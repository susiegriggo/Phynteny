{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from phynteny_utils import train_model\n",
    "import pickle\n",
    "from phynteny_utils import format_data\n",
    "import numpy as np\n",
    "from phynteny_utils import statistics\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# read in some data\n",
    "X = pickle.load(open('test_data/x_mini.pkl', 'rb'))\n",
    "y = pickle.load(open('test_data/y_mini.pkl', 'rb'))\n",
    "\n",
    "for key in list(X.keys()):\n",
    "\n",
    "    m = X.get(key)\n",
    "    #idx = statistics.get_masked(m, 10)\n",
    "    #m[idx][0:10] = np.ones(10)*-1\n",
    "\n",
    "    idx = np.where(m[:,0] == 1)\n",
    "    for i in idx[0]:\n",
    "        m[i][0:10] = np.ones(10)*-1\n",
    "\n",
    "    X[key] = m\n",
    "\n",
    "pickle.dump(X, open('test_data/x_mini_masked.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-e669306a807c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mX_masked\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'test_data/x_mini_masked.pkl'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mstatistics\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_masked\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_masked\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/Phynteny/phynteny_utils/statistics.py\u001B[0m in \u001B[0;36mget_masked\u001B[0;34m(encoding, num_categories)\u001B[0m\n\u001B[1;32m    164\u001B[0m     \"\"\" \n\u001B[1;32m    165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 166\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mnum_categories\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "X_masked = pickle.load(open('test_data/x_mini_masked.pkl', 'rb'))\n",
    "statistics.get_masked(X_masked.get(key),10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([14, 28, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35]),\n array([13, 13,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = X_masked.get(key)\n",
    "np.where(a==-1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "m = train_model.Model()\n",
    "m.parse_masked_data('test_data/x_mini_masked.pkl', 'test_data/y_mini.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susiegrigson/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_5 (Masking)         (None, 120, 16)           0         \n",
      "                                                                 \n",
      " bidirectional_10 (Bidirecti  (None, 120, 200)         93600     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_11 (Bidirecti  (None, 120, 200)         240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 120, 10)          2010      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,410\n",
      "Trainable params: 336,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.2892 - accuracy: 0.8184\n",
      "Epoch 1: val_loss improved from inf to 2.25206, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.84083, saving model to model.rep_0.best_val_loss.h5\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.2892 - accuracy: 0.8184 - val_loss: 2.2521 - val_accuracy: 0.8408\n",
      "Epoch 2/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.2291 - accuracy: 0.8184\n",
      "Epoch 2: val_loss improved from 2.25206 to 2.15232, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 2.2291 - accuracy: 0.8184 - val_loss: 2.1523 - val_accuracy: 0.8408\n",
      "Epoch 3/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.0968 - accuracy: 0.8184\n",
      "Epoch 3: val_loss improved from 2.15232 to 1.87394, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 2.0968 - accuracy: 0.8184 - val_loss: 1.8739 - val_accuracy: 0.8408\n",
      "Epoch 4/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.6267 - accuracy: 0.8184\n",
      "Epoch 4: val_loss improved from 1.87394 to 0.79712, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 1.6267 - accuracy: 0.8184 - val_loss: 0.7971 - val_accuracy: 0.8408\n",
      "Epoch 5/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9075 - accuracy: 0.8184\n",
      "Epoch 5: val_loss did not improve from 0.79712\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.9075 - accuracy: 0.8184 - val_loss: 0.8561 - val_accuracy: 0.8408\n",
      "Epoch 6/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0061 - accuracy: 0.8184\n",
      "Epoch 6: val_loss did not improve from 0.79712\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 1.0061 - accuracy: 0.8184 - val_loss: 0.8132 - val_accuracy: 0.8408\n",
      "Epoch 7/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.8184\n",
      "Epoch 7: val_loss improved from 0.79712 to 0.70840, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.9130 - accuracy: 0.8184 - val_loss: 0.7084 - val_accuracy: 0.8408\n",
      "Epoch 8/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7890 - accuracy: 0.8184\n",
      "Epoch 8: val_loss improved from 0.70840 to 0.68864, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.7890 - accuracy: 0.8184 - val_loss: 0.6886 - val_accuracy: 0.8408\n",
      "Epoch 9/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7776 - accuracy: 0.8184\n",
      "Epoch 9: val_loss did not improve from 0.68864\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7776 - accuracy: 0.8184 - val_loss: 0.7390 - val_accuracy: 0.8408\n",
      "Epoch 10/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7961 - accuracy: 0.8184\n",
      "Epoch 10: val_loss did not improve from 0.68864\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7961 - accuracy: 0.8184 - val_loss: 0.7021 - val_accuracy: 0.8408\n",
      "Epoch 11/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7624 - accuracy: 0.8184\n",
      "Epoch 11: val_loss improved from 0.68864 to 0.66980, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.7624 - accuracy: 0.8184 - val_loss: 0.6698 - val_accuracy: 0.8408\n",
      "Epoch 12/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7525 - accuracy: 0.8184\n",
      "Epoch 12: val_loss improved from 0.66980 to 0.66906, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.7525 - accuracy: 0.8184 - val_loss: 0.6691 - val_accuracy: 0.8408\n",
      "Epoch 13/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.8184\n",
      "Epoch 13: val_loss improved from 0.66906 to 0.66361, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.7536 - accuracy: 0.8184 - val_loss: 0.6636 - val_accuracy: 0.8408\n",
      "Epoch 14/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7420 - accuracy: 0.8184\n",
      "Epoch 14: val_loss improved from 0.66361 to 0.65526, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.7420 - accuracy: 0.8184 - val_loss: 0.6553 - val_accuracy: 0.8408\n",
      "Epoch 15/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.8184\n",
      "Epoch 15: val_loss did not improve from 0.65526\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7311 - accuracy: 0.8184 - val_loss: 0.6645 - val_accuracy: 0.8408\n",
      "Epoch 16/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7322 - accuracy: 0.8184\n",
      "Epoch 16: val_loss did not improve from 0.65526\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7322 - accuracy: 0.8184 - val_loss: 0.6634 - val_accuracy: 0.8408\n",
      "Epoch 17/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7251 - accuracy: 0.8184\n",
      "Epoch 17: val_loss improved from 0.65526 to 0.64429, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.7251 - accuracy: 0.8184 - val_loss: 0.6443 - val_accuracy: 0.8408\n",
      "Epoch 18/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7185 - accuracy: 0.8184\n",
      "Epoch 18: val_loss improved from 0.64429 to 0.63896, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.7185 - accuracy: 0.8184 - val_loss: 0.6390 - val_accuracy: 0.8408\n",
      "Epoch 19/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7190 - accuracy: 0.8184\n",
      "Epoch 19: val_loss improved from 0.63896 to 0.63532, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.7190 - accuracy: 0.8184 - val_loss: 0.6353 - val_accuracy: 0.8408\n",
      "Epoch 20/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.8184\n",
      "Epoch 20: val_loss did not improve from 0.63532\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7113 - accuracy: 0.8184 - val_loss: 0.6378 - val_accuracy: 0.8408\n",
      "Epoch 21/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7106 - accuracy: 0.8184\n",
      "Epoch 21: val_loss did not improve from 0.63532\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 0.7106 - accuracy: 0.8184 - val_loss: 0.6400 - val_accuracy: 0.8408\n",
      "Epoch 22/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7088 - accuracy: 0.8184\n",
      "Epoch 22: val_loss improved from 0.63532 to 0.62955, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.7088 - accuracy: 0.8184 - val_loss: 0.6296 - val_accuracy: 0.8408\n",
      "Epoch 23/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7089 - accuracy: 0.8184\n",
      "Epoch 23: val_loss improved from 0.62955 to 0.62733, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.7089 - accuracy: 0.8184 - val_loss: 0.6273 - val_accuracy: 0.8408\n",
      "Epoch 24/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7062 - accuracy: 0.8184\n",
      "Epoch 24: val_loss did not improve from 0.62733\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7062 - accuracy: 0.8184 - val_loss: 0.6341 - val_accuracy: 0.8408\n",
      "Epoch 25/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7115 - accuracy: 0.8184\n",
      "Epoch 25: val_loss did not improve from 0.62733\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7115 - accuracy: 0.8184 - val_loss: 0.6369 - val_accuracy: 0.8408\n",
      "Epoch 26/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.8184\n",
      "Epoch 26: val_loss improved from 0.62733 to 0.62551, saving model to model.rep_0.best_val_accuracy.h5\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.7076 - accuracy: 0.8184 - val_loss: 0.6255 - val_accuracy: 0.8408\n",
      "Epoch 27/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7107 - accuracy: 0.8184\n",
      "Epoch 27: val_loss did not improve from 0.62551\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.7107 - accuracy: 0.8184 - val_loss: 0.6279 - val_accuracy: 0.8408\n",
      "Epoch 28/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.8184\n",
      "Epoch 28: val_loss did not improve from 0.62551\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 0.7092 - accuracy: 0.8184 - val_loss: 0.6390 - val_accuracy: 0.8408\n",
      "Epoch 29/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7062 - accuracy: 0.8184\n",
      "Epoch 29: val_loss did not improve from 0.62551\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.84083\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.7062 - accuracy: 0.8184 - val_loss: 0.6258 - val_accuracy: 0.8408\n",
      "Epoch 29: early stopping\n",
      "INFO:tensorflow:Assets written to: model.rep_0.final_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.rep_0.final_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_6 (Masking)         (None, 120, 16)           0         \n",
      "                                                                 \n",
      " bidirectional_12 (Bidirecti  (None, 120, 200)         93600     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_13 (Bidirecti  (None, 120, 200)         240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 120, 10)          2010      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,410\n",
      "Trainable params: 336,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.2888 - accuracy: 0.8200\n",
      "Epoch 1: val_loss improved from inf to 2.25162, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.82667, saving model to model.rep_1.best_val_loss.h5\n",
      "3/3 [==============================] - 8s 1s/step - loss: 2.2888 - accuracy: 0.8200 - val_loss: 2.2516 - val_accuracy: 0.8267\n",
      "Epoch 2/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.2262 - accuracy: 0.8200\n",
      "Epoch 2: val_loss improved from 2.25162 to 2.14919, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 2.2262 - accuracy: 0.8200 - val_loss: 2.1492 - val_accuracy: 0.8267\n",
      "Epoch 3/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 2.0849 - accuracy: 0.8200\n",
      "Epoch 3: val_loss improved from 2.14919 to 1.84561, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 2.0849 - accuracy: 0.8200 - val_loss: 1.8456 - val_accuracy: 0.8267\n",
      "Epoch 4/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.5722 - accuracy: 0.8200\n",
      "Epoch 4: val_loss improved from 1.84561 to 0.95782, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 1.5722 - accuracy: 0.8200 - val_loss: 0.9578 - val_accuracy: 0.8267\n",
      "Epoch 5/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9881 - accuracy: 0.8200\n",
      "Epoch 5: val_loss did not improve from 0.95782\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.9881 - accuracy: 0.8200 - val_loss: 1.0486 - val_accuracy: 0.8267\n",
      "Epoch 6/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0251 - accuracy: 0.8200\n",
      "Epoch 6: val_loss did not improve from 0.95782\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 1.0251 - accuracy: 0.8200 - val_loss: 0.9596 - val_accuracy: 0.8267\n",
      "Epoch 7/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.8990 - accuracy: 0.8200\n",
      "Epoch 7: val_loss improved from 0.95782 to 0.80631, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.8990 - accuracy: 0.8200 - val_loss: 0.8063 - val_accuracy: 0.8267\n",
      "Epoch 8/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7752 - accuracy: 0.8200\n",
      "Epoch 8: val_loss improved from 0.80631 to 0.78220, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.7752 - accuracy: 0.8200 - val_loss: 0.7822 - val_accuracy: 0.8267\n",
      "Epoch 9/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7843 - accuracy: 0.8200\n",
      "Epoch 9: val_loss did not improve from 0.78220\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7843 - accuracy: 0.8200 - val_loss: 0.7962 - val_accuracy: 0.8267\n",
      "Epoch 10/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7717 - accuracy: 0.8200\n",
      "Epoch 10: val_loss improved from 0.78220 to 0.75436, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.7717 - accuracy: 0.8200 - val_loss: 0.7544 - val_accuracy: 0.8267\n",
      "Epoch 11/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7349 - accuracy: 0.8200\n",
      "Epoch 11: val_loss did not improve from 0.75436\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.7349 - accuracy: 0.8200 - val_loss: 0.7574 - val_accuracy: 0.8267\n",
      "Epoch 12/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7396 - accuracy: 0.8200\n",
      "Epoch 12: val_loss did not improve from 0.75436\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 136ms/step - loss: 0.7396 - accuracy: 0.8200 - val_loss: 0.7623 - val_accuracy: 0.8267\n",
      "Epoch 13/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7348 - accuracy: 0.8200\n",
      "Epoch 13: val_loss improved from 0.75436 to 0.74690, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.7348 - accuracy: 0.8200 - val_loss: 0.7469 - val_accuracy: 0.8267\n",
      "Epoch 14/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7184 - accuracy: 0.8200\n",
      "Epoch 14: val_loss improved from 0.74690 to 0.74170, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.7184 - accuracy: 0.8200 - val_loss: 0.7417 - val_accuracy: 0.8267\n",
      "Epoch 15/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7168 - accuracy: 0.8200\n",
      "Epoch 15: val_loss did not improve from 0.74170\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7168 - accuracy: 0.8200 - val_loss: 0.7421 - val_accuracy: 0.8267\n",
      "Epoch 16/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7133 - accuracy: 0.8200\n",
      "Epoch 16: val_loss improved from 0.74170 to 0.73536, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.7133 - accuracy: 0.8200 - val_loss: 0.7354 - val_accuracy: 0.8267\n",
      "Epoch 17/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.8200\n",
      "Epoch 17: val_loss did not improve from 0.73536\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.7063 - accuracy: 0.8200 - val_loss: 0.7390 - val_accuracy: 0.8267\n",
      "Epoch 18/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7069 - accuracy: 0.8200\n",
      "Epoch 18: val_loss improved from 0.73536 to 0.73152, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.7069 - accuracy: 0.8200 - val_loss: 0.7315 - val_accuracy: 0.8267\n",
      "Epoch 19/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6990 - accuracy: 0.8200\n",
      "Epoch 19: val_loss improved from 0.73152 to 0.72427, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.6990 - accuracy: 0.8200 - val_loss: 0.7243 - val_accuracy: 0.8267\n",
      "Epoch 20/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.7005 - accuracy: 0.8200\n",
      "Epoch 20: val_loss improved from 0.72427 to 0.72261, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.7005 - accuracy: 0.8200 - val_loss: 0.7226 - val_accuracy: 0.8267\n",
      "Epoch 21/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6960 - accuracy: 0.8200\n",
      "Epoch 21: val_loss did not improve from 0.72261\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6960 - accuracy: 0.8200 - val_loss: 0.7247 - val_accuracy: 0.8267\n",
      "Epoch 22/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6976 - accuracy: 0.8200\n",
      "Epoch 22: val_loss improved from 0.72261 to 0.72247, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.6976 - accuracy: 0.8200 - val_loss: 0.7225 - val_accuracy: 0.8267\n",
      "Epoch 23/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6970 - accuracy: 0.8200\n",
      "Epoch 23: val_loss improved from 0.72247 to 0.71654, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.6970 - accuracy: 0.8200 - val_loss: 0.7165 - val_accuracy: 0.8267\n",
      "Epoch 24/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6956 - accuracy: 0.8200\n",
      "Epoch 24: val_loss did not improve from 0.71654\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6956 - accuracy: 0.8200 - val_loss: 0.7223 - val_accuracy: 0.8267\n",
      "Epoch 25/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.8200\n",
      "Epoch 25: val_loss did not improve from 0.71654\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.6957 - accuracy: 0.8200 - val_loss: 0.7198 - val_accuracy: 0.8267\n",
      "Epoch 26/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6958 - accuracy: 0.8200\n",
      "Epoch 26: val_loss improved from 0.71654 to 0.71594, saving model to model.rep_1.best_val_accuracy.h5\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 148ms/step - loss: 0.6958 - accuracy: 0.8200 - val_loss: 0.7159 - val_accuracy: 0.8267\n",
      "Epoch 27/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.8200\n",
      "Epoch 27: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6948 - accuracy: 0.8200 - val_loss: 0.7231 - val_accuracy: 0.8267\n",
      "Epoch 28/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6955 - accuracy: 0.8200\n",
      "Epoch 28: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6955 - accuracy: 0.8200 - val_loss: 0.7261 - val_accuracy: 0.8267\n",
      "Epoch 29/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6951 - accuracy: 0.8200\n",
      "Epoch 29: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6951 - accuracy: 0.8200 - val_loss: 0.7177 - val_accuracy: 0.8267\n",
      "Epoch 30/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.8200\n",
      "Epoch 30: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.6952 - accuracy: 0.8200 - val_loss: 0.7217 - val_accuracy: 0.8267\n",
      "Epoch 31/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.8200\n",
      "Epoch 31: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6939 - accuracy: 0.8200 - val_loss: 0.7258 - val_accuracy: 0.8267\n",
      "Epoch 32/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.8200\n",
      "Epoch 32: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.6943 - accuracy: 0.8200 - val_loss: 0.7237 - val_accuracy: 0.8267\n",
      "Epoch 33/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6939 - accuracy: 0.8200\n",
      "Epoch 33: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.6939 - accuracy: 0.8200 - val_loss: 0.7198 - val_accuracy: 0.8267\n",
      "Epoch 34/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6948 - accuracy: 0.8200\n",
      "Epoch 34: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6948 - accuracy: 0.8200 - val_loss: 0.7224 - val_accuracy: 0.8267\n",
      "Epoch 35/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6945 - accuracy: 0.8200\n",
      "Epoch 35: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.6945 - accuracy: 0.8200 - val_loss: 0.7226 - val_accuracy: 0.8267\n",
      "Epoch 36/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6938 - accuracy: 0.8200\n",
      "Epoch 36: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.6938 - accuracy: 0.8200 - val_loss: 0.7288 - val_accuracy: 0.8267\n",
      "Epoch 37/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6946 - accuracy: 0.8200\n",
      "Epoch 37: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6946 - accuracy: 0.8200 - val_loss: 0.7206 - val_accuracy: 0.8267\n",
      "Epoch 38/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.8200\n",
      "Epoch 38: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 137ms/step - loss: 0.6943 - accuracy: 0.8200 - val_loss: 0.7221 - val_accuracy: 0.8267\n",
      "Epoch 39/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.8200\n",
      "Epoch 39: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6957 - accuracy: 0.8200 - val_loss: 0.7280 - val_accuracy: 0.8267\n",
      "Epoch 40/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.8200\n",
      "Epoch 40: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6968 - accuracy: 0.8200 - val_loss: 0.7171 - val_accuracy: 0.8267\n",
      "Epoch 41/140\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6941 - accuracy: 0.8200\n",
      "Epoch 41: val_loss did not improve from 0.71594\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.82667\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6941 - accuracy: 0.8200 - val_loss: 0.7285 - val_accuracy: 0.8267\n",
      "Epoch 41: early stopping\n",
      "INFO:tensorflow:Assets written to: model.rep_1.final_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.rep_1.final_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking_7 (Masking)         (None, 120, 16)           0         \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 120, 200)         93600     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_15 (Bidirecti  (None, 120, 200)         240800    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 120, 10)          2010      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,410\n",
      "Trainable params: 336,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-60-7815cf26d190>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_crossValidation\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/PycharmProjects/Phynteny/phynteny_utils/train_model.py\u001B[0m in \u001B[0;36mtrain_crossValidation\u001B[0;34m(self, model_out, history_out, n_splits, epochs, save)\u001B[0m\n\u001B[1;32m    431\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    432\u001B[0m             \u001B[0;31m# use the compile function here\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 433\u001B[0;31m             self.train_model(\n\u001B[0m\u001B[1;32m    434\u001B[0m                 \u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    435\u001B[0m                 \u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Phynteny/phynteny_utils/train_model.py\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(self, X_1, y_1, X_val, y_val, model_out, history_out, epochs, save)\u001B[0m\n\u001B[1;32m    363\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgenerate_LSTM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    364\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 365\u001B[0;31m         history = model.fit(\n\u001B[0m\u001B[1;32m    366\u001B[0m             \u001B[0mX_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    367\u001B[0m             \u001B[0my_1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 65\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     66\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m             \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1562\u001B[0m                         ):\n\u001B[1;32m   1563\u001B[0m                             \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1564\u001B[0;31m                             \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1565\u001B[0m                             \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1566\u001B[0m                                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    913\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    914\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 915\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    916\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    978\u001B[0m         \u001B[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    979\u001B[0m         \u001B[0;31m# stateless function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 980\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    981\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    982\u001B[0m       _, _, filtered_flat_args = (\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2494\u001B[0m       (graph_function,\n\u001B[1;32m   2495\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2496\u001B[0;31m     return graph_function._call_flat(\n\u001B[0m\u001B[1;32m   2497\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[1;32m   2498\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1860\u001B[0m         and executing_eagerly):\n\u001B[1;32m   1861\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1862\u001B[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[1;32m   1863\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[1;32m   1864\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    497\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    498\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 499\u001B[0;31m           outputs = execute.execute(\n\u001B[0m\u001B[1;32m    500\u001B[0m               \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    501\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/phynteny/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m   \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[1;32m     55\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[1;32m     56\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "m.train_crossValidation()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('model.rep_0.best_val_loss.h5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(X[key].reshape(1,120,16))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "            0         1         2         3         4         5         6  \\\n0    0.103874  0.100708  0.099481  0.099897  0.099149  0.099727  0.099246   \n1    0.104389  0.100753  0.099320  0.099821  0.099083  0.099659  0.099196   \n2    0.104804  0.100778  0.099185  0.099760  0.099034  0.099601  0.099176   \n3    0.105137  0.100790  0.099071  0.099713  0.098997  0.099551  0.099175   \n4    0.105407  0.100795  0.098977  0.099678  0.098969  0.099508  0.099186   \n..        ...       ...       ...       ...       ...       ...       ...   \n115  0.105469  0.100358  0.098760  0.099616  0.099147  0.099320  0.099509   \n116  0.105245  0.100299  0.098782  0.099609  0.099205  0.099334  0.099559   \n117  0.104973  0.100237  0.098803  0.099601  0.099272  0.099349  0.099627   \n118  0.104644  0.100173  0.098822  0.099589  0.099353  0.099362  0.099716   \n119  0.104246  0.100112  0.098834  0.099573  0.099448  0.099373  0.099835   \n\n            7         8         9  \n0    0.099062  0.099682  0.099174  \n1    0.099009  0.099659  0.099113  \n2    0.098955  0.099636  0.099072  \n3    0.098904  0.099615  0.099045  \n4    0.098856  0.099596  0.099028  \n..        ...       ...       ...  \n115  0.098996  0.099551  0.099275  \n116  0.099065  0.099572  0.099329  \n117  0.099143  0.099601  0.099394  \n118  0.099227  0.099641  0.099472  \n119  0.099317  0.099695  0.099566  \n\n[120 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.103874</td>\n      <td>0.100708</td>\n      <td>0.099481</td>\n      <td>0.099897</td>\n      <td>0.099149</td>\n      <td>0.099727</td>\n      <td>0.099246</td>\n      <td>0.099062</td>\n      <td>0.099682</td>\n      <td>0.099174</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.104389</td>\n      <td>0.100753</td>\n      <td>0.099320</td>\n      <td>0.099821</td>\n      <td>0.099083</td>\n      <td>0.099659</td>\n      <td>0.099196</td>\n      <td>0.099009</td>\n      <td>0.099659</td>\n      <td>0.099113</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.104804</td>\n      <td>0.100778</td>\n      <td>0.099185</td>\n      <td>0.099760</td>\n      <td>0.099034</td>\n      <td>0.099601</td>\n      <td>0.099176</td>\n      <td>0.098955</td>\n      <td>0.099636</td>\n      <td>0.099072</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.105137</td>\n      <td>0.100790</td>\n      <td>0.099071</td>\n      <td>0.099713</td>\n      <td>0.098997</td>\n      <td>0.099551</td>\n      <td>0.099175</td>\n      <td>0.098904</td>\n      <td>0.099615</td>\n      <td>0.099045</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.105407</td>\n      <td>0.100795</td>\n      <td>0.098977</td>\n      <td>0.099678</td>\n      <td>0.098969</td>\n      <td>0.099508</td>\n      <td>0.099186</td>\n      <td>0.098856</td>\n      <td>0.099596</td>\n      <td>0.099028</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>0.105469</td>\n      <td>0.100358</td>\n      <td>0.098760</td>\n      <td>0.099616</td>\n      <td>0.099147</td>\n      <td>0.099320</td>\n      <td>0.099509</td>\n      <td>0.098996</td>\n      <td>0.099551</td>\n      <td>0.099275</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>0.105245</td>\n      <td>0.100299</td>\n      <td>0.098782</td>\n      <td>0.099609</td>\n      <td>0.099205</td>\n      <td>0.099334</td>\n      <td>0.099559</td>\n      <td>0.099065</td>\n      <td>0.099572</td>\n      <td>0.099329</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>0.104973</td>\n      <td>0.100237</td>\n      <td>0.098803</td>\n      <td>0.099601</td>\n      <td>0.099272</td>\n      <td>0.099349</td>\n      <td>0.099627</td>\n      <td>0.099143</td>\n      <td>0.099601</td>\n      <td>0.099394</td>\n    </tr>\n    <tr>\n      <th>118</th>\n      <td>0.104644</td>\n      <td>0.100173</td>\n      <td>0.098822</td>\n      <td>0.099589</td>\n      <td>0.099353</td>\n      <td>0.099362</td>\n      <td>0.099716</td>\n      <td>0.099227</td>\n      <td>0.099641</td>\n      <td>0.099472</td>\n    </tr>\n    <tr>\n      <th>119</th>\n      <td>0.104246</td>\n      <td>0.100112</td>\n      <td>0.098834</td>\n      <td>0.099573</td>\n      <td>0.099448</td>\n      <td>0.099373</td>\n      <td>0.099835</td>\n      <td>0.099317</td>\n      <td>0.099695</td>\n      <td>0.099566</td>\n    </tr>\n  </tbody>\n</table>\n<p>120 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(a[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       ...,\n       [1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0]])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[key]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
