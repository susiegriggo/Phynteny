[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![FAME Lab](https://img.shields.io/badge/Bioinformatics-EdwardsLab-03A9F4)](https://fame.flinders.edu.au/)


# Phynteny

## Phynteny: Synteny-based annotation of viral genes 

Phynteny uses a Long-Short Term Memory model to predict the function of unknown phage genes. <br> 

Phynteny is still a work in progress and the LSTM model has not yet been optimised. Use with caution! 

## Installation 

```
git clone https://github.com/susiegriggo/Phynteny
conda create -n phynteny
conda activate phynteny 
cd Phynteny 
pip install . 
```

## Usage 

Phynteny takes a genbank file containing PHROG annotations as input. If you phage is not yet in this format, [pharokka](https://github.com/gbouras13/pharokka) can take your phage (in fasta format) to a genbank file with PHROG annotations.  

**Reccomended:**  
```
phynteny your_phage.gbk  -o your_phage_reannotated.gbk 
```

If you wish to specify your own LSTM model and thresholds, you can run: 
```
phynteny your_phage.gbk -o your_phage_reannotated.gbk -m your_model.h5 -t custom_thresholds.pkl 
```

where ```custom_thresholds.pkl``` is a dictionary contanining a softmax threshold for each of the nine PHROG categories. 

## Training Phynteny 
Phynteny has already been trained for you on a [massive dataset of prophages](https://www.biorxiv.org/content/10.1101/2023.04.20.537752v1.abstract)!  
However, If you feel inclined to train Phynteny yourself you can. <br>

Phynteny is trained using genbank files containing PHROG annotations such as those generated by pharokka using scripts located in the /scripts directory. 
Youcan see the options for training by running the command ```python scripts/generate_training_data.py  --help```

```
Options:
  -i, --input_data PATH          Text file containing genbank files to build
                                 model  [required]
  -m, --maximum_genes INTEGER    Specify the maximum number of genes in each
                                 genome
  -g, --gene_categories INTEGER  Specify the minimum number of categories in
                                 each genome
  -p, --prefix TEXT              Prefix for the output files
  --help                         Show this message and exit.
 ```

Once you've generated your training data, you can train a Phynteny model by running the ```scripts/train_model/py``` script. There are several options for the model which you can view by running the command 
```python scripts/train_model.py --help```

```
Options:
  -x, --x_path TEXT               File path to X training data
  -y, --y_path TEXT               File path to y training data
  -ml, --max_length INTEGER       Maximum length of a phage genome
  -l, --layers INTEGER            Number of hidden layers to use in model.
                                  Default = 1
  -m, --memory_cells INTEGER      Number of memory cells to train the model.
                                  Default = 100
  -b, --batch_size INTEGER        Batch size to use for training the model
  -dr, --dropout FLOAT            Dropout applied to prevent overfitting
  -a, --activation [tanh|sigmoid|relu]
                                  activtion function applied to the input and
                                  hidden layers. Must be one of ['tanh',
                                  'sigmoid', 'relu']
  -opt, --optimizer [adam|rmsprop|adagrad|sgd]
                                  Optimization function. Must be one of
                                  ['adam', 'rmsprop', 'adagrad', 'sgd']
  -lr, --learning_rate FLOAT      Learning rate applied to opimization
                                  function
  -p, --patience INTEGER          number of epochs with no improvement after
                                  which training will be stopped
  -md, --min_delta FLOAT          minimum change in validation loss considered
                                  an improvement
  -o, --model_out TEXT            prefix of the model output
  -ho, --history_out TEXT         prefix of history dictionary output
  -k, --k_folds INTEGER           Number of folds to use for k-fold cross-
                                  validation
  -e, --epochs INTEGER            Maximum number of epochs to train for
  -l1, --l1_regularize FLOAT      L1 regularization. Default=0
  -l1, --l2_regularize FLOAT      L2 regularization. Default=0
  -ki, --kernel_initializer TEXT  kernel initializer
  --help                          Show this message and exit.
```

This command generates training data including prophages with a maximum of 120 genes where each contains at least four different PHROG categories. The output data is separated into 11 different chunks which can be used for training with k-fold validation. 

**WARNING** Without a GPU training will take a very very long time! 

Scripts are included to evaluate the performance of each k-fold and the sum of multiple models at ```scripts/per_fold_test.py``` and ```scripts/per_model_test.py```. Notebooks are included to generate figures describing model performance and to select the optimal thresholds for your custom model. 

## Bugs and Suggestions 
If you break Phynteny or would like to make any suggestions please open an issue or email me at susie.grigson@flinders.edu.au.  

## Citation 
If you use pharokka to first annotate your phage please site it as well! 
